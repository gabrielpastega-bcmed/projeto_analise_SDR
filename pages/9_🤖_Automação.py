"""
PÃ¡gina de AutomaÃ§Ã£o - Monitoramento de GitHub Actions.

Exibe status e histÃ³rico das execuÃ§Ãµes automatizadas de anÃ¡lise.
"""

from datetime import datetime, timedelta

import requests
import streamlit as st

from src.auth.auth_manager import AuthManager

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(page_title="AutomaÃ§Ã£o", page_icon="ðŸ¤–", layout="wide")

# Verificar autenticaÃ§Ã£o
if not AuthManager.is_authenticated():
    st.warning("âš ï¸ VocÃª precisa fazer login para acessar esta pÃ¡gina.")
    st.stop()

# ConfiguraÃ§Ãµes do repositÃ³rio
REPO_OWNER = "gabrielpastega-bcmed"
REPO_NAME = "projeto_analise_SDR"
WORKFLOWS = {
    "weekly": "weekly_analysis.yml",
    "manual": "manual_analysis.yml",
    "monitoring": "monitoring.yml",
}

# Header
st.title("ðŸ¤– AutomaÃ§Ã£o de AnÃ¡lises")
st.markdown("Monitoramento das execuÃ§Ãµes automatizadas via GitHub Actions")

# ========================================
# FunÃ§Ãµes de API
# ========================================


@st.cache_data(ttl=300)  # Cache por 5 minutos
def get_workflow_runs(workflow_name: str, limit: int = 30) -> list:
    """Busca Ãºltimas execuÃ§Ãµes de um workflow especÃ­fico."""
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/actions/workflows/{workflow_name}/runs"
    headers = {"Accept": "application/vnd.github+json"}

    try:
        response = requests.get(url, headers=headers, params={"per_page": limit}, timeout=10)
        if response.status_code == 200:
            return response.json().get("workflow_runs", [])
        else:
            st.error(f"Erro ao buscar dados: HTTP {response.status_code}")
            return []
    except Exception as e:
        st.error(f"Erro ao conectar com GitHub API: {e}")
        return []


def format_duration(start: str, end: str) -> str:
    """Calcula duraÃ§Ã£o entre dois timestamps ISO."""
    try:
        start_dt = datetime.fromisoformat(start.replace("Z", "+00:00"))
        end_dt = datetime.fromisoformat(end.replace("Z", "+00:00"))
        duration = (end_dt - start_dt).total_seconds()
        return f"{duration / 60:.1f} min"
    except Exception:
        return "N/A"


def format_timestamp(timestamp: str) -> str:
    """Formata timestamp ISO para formato brasileiro."""
    try:
        dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
        # Converter para BRT (UTC-3)
        dt_brt = dt - timedelta(hours=3)
        return dt_brt.strftime("%d/%m/%Y %H:%M")
    except Exception:
        return timestamp


def get_status_emoji(status: str) -> str:
    """Retorna emoji baseado no status."""
    status_map = {
        "success": "âœ…",
        "failure": "âŒ",
        "cancelled": "âš ï¸",
        "in_progress": "ðŸ”„",
        "queued": "â³",
    }
    return status_map.get(status, "â“")


# ========================================
# Status Card - Ãšltima ExecuÃ§Ã£o Semanal
# ========================================

st.subheader("ðŸ“Š Status Atual")

weekly_runs = get_workflow_runs(WORKFLOWS["weekly"], limit=1)

if weekly_runs:
    last_run = weekly_runs[0]

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status = last_run.get("conclusion") or "in_progress"
        status_emoji = get_status_emoji(status)
        status_text = "Em execuÃ§Ã£o" if status == "in_progress" else status.title()

        if status == "success":
            st.metric("Status", f"{status_emoji} Sucesso", delta="Ãšltima execuÃ§Ã£o")
        elif status == "failure":
            st.metric(
                "Status",
                f"{status_emoji} Falha",
                delta="Requer atenÃ§Ã£o",
                delta_color="inverse",
            )
        else:
            st.metric("Status", f"{status_emoji} {status_text}")

    with col2:
        created = last_run["created_at"]
        st.metric("Ãšltima ExecuÃ§Ã£o", format_timestamp(created))

    with col3:
        if last_run.get("updated_at"):
            duration = format_duration(last_run["created_at"], last_run["updated_at"])
            st.metric("DuraÃ§Ã£o", duration)
        else:
            st.metric("DuraÃ§Ã£o", "Em andamento...")

    with col4:
        st.link_button("ðŸ“‹ Ver Logs", last_run["html_url"], use_container_width=True)

    # Detalhes adicionais
    with st.expander("â„¹ï¸ Mais InformaÃ§Ãµes"):
        col_a, col_b = st.columns(2)

        with col_a:
            st.write(f"**Run ID:** #{last_run['run_number']}")
            st.write(f"**Branch:** {last_run['head_branch']}")
            st.write(f"**Trigger:** {last_run['event']}")

        with col_b:
            st.write(f"**Commit:** `{last_run['head_sha'][:7]}`")
            st.write(f"**Actor:** @{last_run['triggering_actor']['login']}")

else:
    st.info("ðŸ“­ Nenhuma execuÃ§Ã£o automÃ¡tica encontrada ainda.")

st.divider()

# ========================================
# HistÃ³rico de ExecuÃ§Ãµes
# ========================================

st.subheader("ðŸ“œ HistÃ³rico de ExecuÃ§Ãµes")

# Tabs para diferentes workflows
tab1, tab2, tab3 = st.tabs(["ðŸ—“ï¸ Semanal", "âš¡ Manual", "ðŸ“Š Monitoring"])

with tab1:
    st.caption("ExecuÃ§Ãµes automÃ¡ticas (toda segunda-feira 6AM UTC)")

    runs = get_workflow_runs(WORKFLOWS["weekly"], limit=30)

    if runs:
        # Preparar dados para tabela
        table_data = []
        for run in runs:
            table_data.append(
                {
                    "Status": get_status_emoji(run.get("conclusion") or "in_progress"),
                    "Data/Hora": format_timestamp(run["created_at"]),
                    "DuraÃ§Ã£o": format_duration(run["created_at"], run.get("updated_at", run["created_at"])),
                    "Trigger": run["event"],
                    "Run": f"#{run['run_number']}",
                    "Logs": run["html_url"],
                }
            )

        st.dataframe(
            table_data,
            use_container_width=True,
            column_config={
                "Logs": st.column_config.LinkColumn("Logs", display_text="Ver"),
            },
            hide_index=True,
        )

        # EstatÃ­sticas
        success_count = sum(1 for r in runs if r.get("conclusion") == "success")
        total = len(runs)

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total de Runs", total)
        with col2:
            st.metric("Sucessos", success_count)
        with col3:
            success_rate = (success_count / total * 100) if total > 0 else 0
            st.metric("Taxa de Sucesso", f"{success_rate:.1f}%")
    else:
        st.info("Nenhum histÃ³rico disponÃ­vel.")

with tab2:
    st.caption("ExecuÃ§Ãµes manuais via GitHub Actions")

    manual_runs = get_workflow_runs(WORKFLOWS["manual"], limit=30)

    if manual_runs:
        table_data = []
        for run in manual_runs:
            table_data.append(
                {
                    "Status": get_status_emoji(run.get("conclusion") or "in_progress"),
                    "Data/Hora": format_timestamp(run["created_at"]),
                    "DuraÃ§Ã£o": format_duration(run["created_at"], run.get("updated_at", run["created_at"])),
                    "Por": f"@{run['triggering_actor']['login']}",
                    "Run": f"#{run['run_number']}",
                    "Logs": run["html_url"],
                }
            )

        st.dataframe(
            table_data,
            use_container_width=True,
            column_config={"Logs": st.column_config.LinkColumn("Logs", display_text="Ver")},
            hide_index=True,
        )
    else:
        st.info("Nenhuma execuÃ§Ã£o manual registrada.")

with tab3:
    st.caption("VerificaÃ§Ãµes diÃ¡rias de monitoramento")

    monitoring_runs = get_workflow_runs(WORKFLOWS["monitoring"], limit=30)

    if monitoring_runs:
        table_data = []
        for run in monitoring_runs:
            table_data.append(
                {
                    "Status": get_status_emoji(run.get("conclusion") or "in_progress"),
                    "Data": format_timestamp(run["created_at"]),
                    "Resultado": run.get("conclusion", "running").title(),
                    "Run": f"#{run['run_number']}",
                    "Logs": run["html_url"],
                }
            )

        st.dataframe(
            table_data,
            use_container_width=True,
            column_config={"Logs": st.column_config.LinkColumn("Logs", display_text="Ver")},
            hide_index=True,
        )
    else:
        st.info("Nenhum monitoramento registrado.")

st.divider()

# ========================================
# Executar Manualmente
# ========================================

st.subheader("âš¡ Executar AnÃ¡lise Manualmente")

st.info(
    """
    **InstruÃ§Ãµes:**

    Para executar uma anÃ¡lise manual, use uma das opÃ§Ãµes abaixo:

    **OpÃ§Ã£o 1: GitHub UI (Recomendado)**
    1. VÃ¡ para [Actions > Manual Chat Analysis](https://github.com/{}/{}/actions/workflows/manual_analysis.yml)
    2. Clique em "Run workflow"
    3. Configure os parÃ¢metros
    4. Clique em "Run workflow" (botÃ£o verde)
    """.format(REPO_OWNER, REPO_NAME)
)

with st.expander("ðŸ“‹ OpÃ§Ã£o 2: GitHub CLI", expanded=False):
    col1, col2 = st.columns(2)

    with col1:
        max_chats = st.number_input("MÃ¡ximo de chats", min_value=10, max_value=10000, value=1000, step=100)

    with col2:
        week_start = st.date_input("InÃ­cio da semana (opcional)", value=None)

    save_bq = st.checkbox("Salvar no BigQuery", value=True)

    week_param = f"-f week_start={week_start.strftime('%Y-%m-%d')}" if week_start else ""

    command = f"""gh workflow run manual_analysis.yml \\
  -f max_chats={max_chats} \\
  {week_param} \\
  -f save_to_bigquery={str(save_bq).lower()}"""

    st.code(command, language="bash")
    st.caption("ðŸ’¡ Requer [GitHub CLI](https://cli.github.com/) instalado e autenticado")

st.divider()

# ========================================
# DocumentaÃ§Ã£o
# ========================================

with st.expander("ðŸ“š DocumentaÃ§Ã£o de Secrets", expanded=False):
    st.markdown(
        """
    ### Secrets Configurados

    Os seguintes secrets devem estar configurados no GitHub para os workflows funcionarem:

    **Essenciais:**
    - `GEMINI_API_KEY` - API key do Google Gemini
    - `BIGQUERY_PROJECT_ID` - ID do projeto GCP
    - `BIGQUERY_DATASET_ID` - Nome do dataset
    - `GCP_SA_KEY` - Service Account JSON completo

    **NotificaÃ§Ãµes:**
    - `MAIL_USERNAME` - Email para enviar notificaÃ§Ãµes
    - `MAIL_PASSWORD` - Senha ou App Password
    - `NOTIFICATION_EMAIL` - Email para receber alertas

    **Opcional:**
    - `REDIS_URL` - URL do Redis para caching

    ðŸ“„ [Ver documentaÃ§Ã£o completa](https://github.com/{}/{}/blob/main/.github/SECRETS.md)
    """.format(REPO_OWNER, REPO_NAME)
    )

# Footer
st.caption(
    """
ðŸ¤– **AutomaÃ§Ã£o powered by GitHub Actions**
| ðŸ“Š Dados atualizados a cada 5 minutos
| [Ver Workflows](https://github.com/{}/{}/actions)
""".format(REPO_OWNER, REPO_NAME)
)
